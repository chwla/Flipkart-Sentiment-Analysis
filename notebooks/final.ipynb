{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c94b9b8",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b337897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c799d",
   "metadata": {},
   "source": [
    "Loading and Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee4068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         ProductName     Price  Rate  \\\n",
      "0  Candes 12 L Room/Personal Air Cooler?ÿ?ÿ(White...  ??3,999   5.0   \n",
      "1  Candes 12 L Room/Personal Air Cooler?ÿ?ÿ(White...  ??3,999   5.0   \n",
      "2  Candes 12 L Room/Personal Air Cooler?ÿ?ÿ(White...  ??3,999   3.0   \n",
      "3  Candes 12 L Room/Personal Air Cooler?ÿ?ÿ(White...  ??3,999   1.0   \n",
      "4  Candes 12 L Room/Personal Air Cooler?ÿ?ÿ(White...  ??3,999   3.0   \n",
      "\n",
      "            Review                                            Summary  \\\n",
      "0           Super!  Great cooler.. excellent air flow and for this...   \n",
      "1          Awesome             Best budget 2 fit cooler. Nice cooling   \n",
      "2             Fair  The quality is good but the power of air is de...   \n",
      "3  Useless product                 Very bad product it's a only a fan   \n",
      "4             Fair                                      Ok ok product   \n",
      "\n",
      "                                           full_text  sentiment  fake_review  \n",
      "0  Great cooler.. excellent air flow and for this...          2            0  \n",
      "1     Best budget 2 fit cooler. Nice cooling Awesome          2            0  \n",
      "2  The quality is good but the power of air is de...          1            0  \n",
      "3  Very bad product it's a only a fan Useless pro...          0            0  \n",
      "4                                 Ok ok product Fair          1            0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/flipkart_product.csv', encoding='latin1')\n",
    "\n",
    "#Cleaning Review and Summary\n",
    "df['Review'] = df['Review'].astype(str).str.replace('\\n', ' ', regex=False).str.strip()\n",
    "df['Summary'] = df['Summary'].astype(str).str.replace('\\n', ' ', regex=False).str.strip()\n",
    "\n",
    "df['full_text'] = df['Summary'] + ' ' + df['Review']\n",
    "\n",
    "def sentiment_label(rate):\n",
    "    if rate <= 2:\n",
    "        return 0 #Negative\n",
    "    elif rate == 3:\n",
    "        return 1 #Neutral\n",
    "    else:\n",
    "        return 2 #Positive\n",
    "    \n",
    "df['Rate'] = pd.to_numeric(df['Rate'], errors='coerce')\n",
    "df['sentiment'] = df['Rate'].apply(sentiment_label)\n",
    "\n",
    "#Fake review heuristic\n",
    "df['fake_review'] = ((df['full_text'].str.len() < 30) & (df['Rate'] >= 4)).astype(int)\n",
    "\n",
    "#Saved cleaned csv file\n",
    "df.to_csv('../data/cleaned_data.csv', index=False)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8dbdb8",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a72405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['full_text'].values\n",
    "y_sentiment = df['sentiment'].values\n",
    "y_fake = df['fake_review'].values\n",
    "\n",
    "X_train, X_test, y_sent_train, y_sent_test, y_fake_train, y_fake_test = train_test_split(\n",
    "    X, y_sentiment, y_fake, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1915a",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72a6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60889b5",
   "metadata": {},
   "source": [
    "Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "905b2708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chwla/Desktop/flipkart_sentiment_analysis/venv/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-09-22 16:22:34.733545: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 102ms/step - accuracy: 0.7804 - loss: 0.6809 - val_accuracy: 0.7837 - val_loss: 0.6653\n",
      "Epoch 2/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 103ms/step - accuracy: 0.7808 - loss: 0.6750 - val_accuracy: 0.7839 - val_loss: 0.6649\n",
      "Epoch 3/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 101ms/step - accuracy: 0.8393 - loss: 0.5391 - val_accuracy: 0.8544 - val_loss: 0.4997\n",
      "Epoch 4/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 119ms/step - accuracy: 0.9378 - loss: 0.2120 - val_accuracy: 0.9818 - val_loss: 0.0649\n",
      "Epoch 5/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 119ms/step - accuracy: 0.9812 - loss: 0.0689 - val_accuracy: 0.9836 - val_loss: 0.0574\n"
     ]
    }
   ],
   "source": [
    "model_sentiment = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_sentiment.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_sent = model_sentiment.fit(\n",
    "    X_train_pad, y_sent_train, \n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test_pad, y_sent_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b9e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 125ms/step - accuracy: 0.6412 - loss: 0.6480 - val_accuracy: 0.6704 - val_loss: 0.6296\n",
      "Epoch 2/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 126ms/step - accuracy: 0.7086 - loss: 0.5982 - val_accuracy: 0.7034 - val_loss: 0.5997\n",
      "Epoch 3/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 106ms/step - accuracy: 0.7183 - loss: 0.5710 - val_accuracy: 0.9206 - val_loss: 0.2137\n",
      "Epoch 4/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 111ms/step - accuracy: 0.9409 - loss: 0.1567 - val_accuracy: 0.9682 - val_loss: 0.0928\n",
      "Epoch 5/5\n",
      "\u001b[1m2374/2374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 129ms/step - accuracy: 0.9662 - loss: 0.0990 - val_accuracy: 0.9717 - val_loss: 0.0821\n"
     ]
    }
   ],
   "source": [
    "model_fake = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model_fake.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_fake = model_fake.fit(\n",
    "    X_train_pad, y_fake_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test_pad, y_fake_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79880c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_sentiment.save('../models/sentiment_model.h5')\n",
    "model_fake.save('../models/fake_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db9813",
   "metadata": {},
   "source": [
    "Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "282846b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step\n",
      "\n",
      "Sentiment Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      5133\n",
      "           1       0.97      0.87      0.92      3081\n",
      "           2       0.99      1.00      0.99     29761\n",
      "\n",
      "    accuracy                           0.98     37975\n",
      "   macro avg       0.98      0.95      0.96     37975\n",
      "weighted avg       0.98      0.98      0.98     37975\n",
      "\n",
      "Sentiment Confusion Matrix:\n",
      "[[ 5023    23    87]\n",
      " [  103  2676   302]\n",
      " [   48    59 29654]]\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step\n",
      "\n",
      "Fake Review Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98     23356\n",
      "           1       0.95      0.97      0.96     14619\n",
      "\n",
      "    accuracy                           0.97     37975\n",
      "   macro avg       0.97      0.97      0.97     37975\n",
      "weighted avg       0.97      0.97      0.97     37975\n",
      "\n",
      "Fake Review Confusion Matrix:\n",
      "[[22654   702]\n",
      " [  374 14245]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test, task=\"Task\", is_binary=False):\n",
    "    preds = model.predict(X_test)\n",
    "    if is_binary:\n",
    "        pred_classes = (preds > 0.5).astype(\"int32\")\n",
    "    else:\n",
    "        pred_classes = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(f\"\\n{task} Classification Report:\")\n",
    "    print(classification_report(y_test, pred_classes))\n",
    "\n",
    "    print(f\"{task} Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, pred_classes))\n",
    "\n",
    "# Run evaluations\n",
    "evaluate_model(model_sentiment, X_test_pad, y_sent_test, task=\"Sentiment\", is_binary=False)\n",
    "evaluate_model(model_fake, X_test_pad, y_fake_test, task=\"Fake Review\", is_binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
